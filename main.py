import nltk
from newspaper import Article
import streamlit as st
from emotion import emotion_analyze
from clickbait_detector import BERTClickbaitDetector
import torch
import torch.nn as nn
import numpy as np
import re

@st.cache_resource
def download_nltk_data():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('tokenizers/punkt_tab')
    except LookupError:
        nltk.download('punkt_tab')

# ì•± ì‹œì‘ ì‹œ ë‹¤ìš´ë¡œë“œ
download_nltk_data()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€",
    page_icon="ğŸ“°",
    layout="wide"
)

# í† í¬ë‚˜ì´ì €
class SimpleTokenizer:
    def morphs(self, text):
        return re.findall(r'\w+', text)

tokenizer = SimpleTokenizer()

# ìš”ì•½ ëª¨ë¸ ì •ì˜
class FastSummarizer(nn.Module):
    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim//2, bidirectional=True, 
                           batch_first=True, num_layers=1)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        b, n, s = x.size()
        x = x.view(b * n, s)
        emb = self.embedding(x)
        mask = (x != 0).unsqueeze(-1).float()
        sent_repr = (emb * mask).sum(1) / (mask.sum(1) + 1e-8)
        sent_repr = sent_repr.view(b, n, -1)
        doc_repr, _ = self.lstm(sent_repr)
        scores = self.fc(doc_repr).squeeze(-1)
        return scores

# ìºì‹œëœ ìš”ì•½ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_summarizer_model():
    try:
        vocab = torch.load('vocab.pt')
        model = FastSummarizer(len(vocab), embed_dim=100, hidden_dim=128)
        model.load_state_dict(torch.load('fast_model.pt', map_location='cpu'))
        model.eval()
        return model, vocab
    except:
        return None, None

# ì¤‘ìš” ë¬¸ì¥ ì˜ˆì¸¡
def predict_key_sentences(model, vocab, text, top_k=3, max_sent_len=30, max_doc_len=20):
    if model is None or vocab is None:
        return []
    
    try:
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) < 3:
            return []
        
        content_sentences = sentences[:max_doc_len]
        
        # ì¸ì½”ë”©
        encoded = []
        for sent in content_sentences:
            tokens = tokenizer.morphs(str(sent))[:max_sent_len]
            indices = [vocab.get(t, 1) for t in tokens]
            indices += [0] * (max_sent_len - len(indices))
            encoded.append(indices)
        
        num_sents = len(encoded)
        while len(encoded) < max_doc_len:
            encoded.append([0] * max_sent_len)
        
        with torch.no_grad():
            x = torch.LongTensor(encoded).unsqueeze(0)
            scores = model(x).squeeze().numpy()
        
        # ìƒìœ„ kê°œ ë¬¸ì¥ ì„ íƒ
        valid_scores = scores[:num_sents]
        top_idx = np.argsort(valid_scores)[-top_k:][::-1]
        
        return [content_sentences[i] for i in top_idx]
    except:
        return []

# ê¸°ì‚¬ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def get_article_text(url):
    try:
        article = Article(url, language='ko')
        article.download()
        article.parse()
        return article.title, article.text
    except Exception as e:
        st.error(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None, None

# ì ìˆ˜ ê³„ì‚°
def calculate_positive_scores(clickbait_prob, emotion_score):
    reliability_score = max(0, 100 - clickbait_prob)
    objectivity_score = max(0, 100 - emotion_score)
    quality_score = reliability_score * 0.7 + objectivity_score * 0.3
    
    return reliability_score, objectivity_score, quality_score

# ë‰´ìŠ¤ ì‹ ë¢°ë“±ê¸‰ í‰ê°€
def get_quality_grade(score):
    if score >= 85:
        return "ğŸ† ìµœìš°ìˆ˜", "success"
    elif score >= 70:
        return "ğŸ¥‡ ìš°ìˆ˜", "success"
    elif score >= 55:
        return "ğŸ¥ˆ ì–‘í˜¸", "warning"
    elif score >= 40:
        return "ğŸ¥‰ ë³´í†µ", "warning"
    else:
        return "âš ï¸ ì£¼ì˜", "error"


# Streamlit UI
st.title("ğŸ“° ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ")
st.caption("BERT ê¸°ë°˜ AIê°€ ë‰´ìŠ¤ì˜ ì‹ ë¢°ë„ì™€ ê°ê´€ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.write("""
    1. ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”
    2. 'í’ˆì§ˆ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
    3. ì¢…í•© ì ìˆ˜ì™€ ì„¸ë¶€ ë¶„ì„ì„ í™•ì¸í•˜ì„¸ìš”
    
    **âœ¨ í‰ê°€ ê¸°ì¤€:**
    - **ì‹ ë¢°ë„ (70%)**: ë‚šì‹œì„±ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    - **ê°ê´€ì„± (30%)**: ê°ì •ì  í‘œí˜„ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
    """)
    
    st.header("ğŸ† ë“±ê¸‰ ê¸°ì¤€")
    st.write("""
    - **ìµœìš°ìˆ˜ (85-100ì )**: ğŸ†
    - **ìš°ìˆ˜ (70-84ì )**: ğŸ¥‡
    - **ì–‘í˜¸ (55-69ì )**: ğŸ¥ˆ
    - **ë³´í†µ (40-54ì )**: ğŸ¥‰
    - **ì£¼ì˜ (0-39ì )**: âš ï¸
    """)

# ìš”ì•½ ëª¨ë¸ ë¡œë“œ
summary_model, vocab = load_summarizer_model()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'results' not in st.session_state:
    st.session_state.results = {}

# ë©”ì¸ ì…ë ¥ ì˜ì—­
url = st.text_input(
    "ë‰´ìŠ¤ ê¸°ì‚¬ URL ì…ë ¥",
    placeholder="https://news.example.com/article/12345",
    help="ë„¤ì´ë²„, ë‹¤ìŒ ë“±ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”"
)

if st.button("ğŸ” í’ˆì§ˆ ë¶„ì„í•˜ê¸°", type="primary"):
    if url:
        with st.spinner('ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
            try:
                # ê¸°ì‚¬ ì¶”ì¶œ
                st.info("ğŸ“° ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                title, text = get_article_text(url)
                
                if not title or not text:
                    st.error("ê¸°ì‚¬ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    st.stop()
                else:
                    st.success("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
                
                # ë‚šì‹œì„± ë¶„ì„
                st.info("ğŸ£ ì‹ ë¢°ë„ ë¶„ì„ ì¤‘...")
                detector = BERTClickbaitDetector(model_path="clickbait_detector_bert1.pt", model_name='klue/bert-base', max_length=256)
                result = detector.predict(title, text)
                clickbait_prob = result['clickbait_probability'] * 100
                st.success("âœ…  ì‹ ë¢°ë„ ë¶„ì„ ì™„ë£Œ!")
                
                # ê°ì • ë¶„ì„
                st.info("ğŸ˜Š ê°ê´€ì„± ë¶„ì„ ì¤‘...")
                emotion, e_score = emotion_analyze(text)
                st.success("âœ…  ê°ê´€ì„± ë¶„ì„ ì™„ë£Œ!")
                
                # ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ
                key_sentences = []
                if summary_model is not None:
                    st.info("âœ¨ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
                    key_sentences = predict_key_sentences(summary_model, vocab, text)
                    st.success("âœ… ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ì™„ë£Œ!")
                
                # ì ìˆ˜ ê³„ì‚°
                reliability_score, objectivity_score, quality_score = calculate_positive_scores(
                    clickbait_prob, e_score
                )
                
                # ê²°ê³¼ ì €ì¥ 
                st.session_state.results = {
                    'title': title,
                    'text': text,
                    'clickbait_prob': clickbait_prob,
                    'emotion': emotion,
                    'e_score': e_score,
                    'reliability_score': reliability_score,
                    'objectivity_score': objectivity_score,
                    'quality_score': quality_score,
                    'url': url,
                    'method': result['method'],
                    'key_sentences': key_sentences
                }
                st.session_state.analysis_done = True
                st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.session_state.analysis_done = False
    else:
        st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")


if st.session_state.analysis_done and st.session_state.results:
    results = st.session_state.results
    
    st.divider()
    st.header("ğŸ“Š í’ˆì§ˆ ë¶„ì„ ê²°ê³¼")
    
    # ë©”ì¸ í’ˆì§ˆ ì ìˆ˜
    quality_grade, grade_type = get_quality_grade(results['quality_score'])
    
    col_main = st.columns(1)[0]
    with col_main:
        if grade_type == "success":
            st.success(f"**ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.0f}ì ** {quality_grade}")
        elif grade_type == "warning":
            st.warning(f"**ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.0f}ì ** {quality_grade}")
        else:
            st.error(f"**ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {results['quality_score']:.0f}ì ** {quality_grade}")
    
    # ì„¸ë¶€ ì ìˆ˜
    st.subheader("ğŸ… ì„¸ë¶€ í‰ê°€ ì ìˆ˜")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        reliability_delta = "ë†’ìŒ" if results['reliability_score'] > 70 else "ë³´í†µ" if results['reliability_score'] > 50 else "ë‚®ìŒ"
        st.metric(
            "ğŸ›¡ï¸ ì‹ ë¢°ë„ ì ìˆ˜",
            f"{results['reliability_score']:.0f}ì ",
            delta=reliability_delta,
            delta_color="normal"
        )
    
    with col2:
        objectivity_delta = "ë†’ìŒ" if results['objectivity_score'] > 70 else "ë³´í†µ" if results['objectivity_score'] > 50 else "ë‚®ìŒ"
        st.metric(
            "âš–ï¸ ê°ê´€ì„± ì ìˆ˜",
            f"{results['objectivity_score']:.0f}ì ",
            delta=objectivity_delta,
            delta_color="normal"
        )
    
    with col3:
        st.metric(
            "ğŸ“ˆ ì¢…í•© í’ˆì§ˆ",
            f"{results['quality_score']:.0f}ì ",
            delta=quality_grade.split()[1] if len(quality_grade.split()) > 1 else "",
            delta_color="normal"
        )
    
    # ìƒì„¸ ë¶„ì„
    st.subheader("ğŸ“‹ ìƒì„¸ ë¶„ì„")
    
    col_detail1, col_detail2 = st.columns(2)
    
    with col_detail1:
        st.write("**ğŸ£ ë‚šì‹œì„± ë¶„ì„**")
        if results['clickbait_prob'] < 30:
            st.success(f"ë‚šì‹œì„± í™•ë¥ : {results['clickbait_prob']:.1f}% (ì‹ ë¢°í•  ë§Œí•¨)")
        elif results['clickbait_prob'] < 70:
            st.warning(f"ë‚šì‹œì„± í™•ë¥ : {results['clickbait_prob']:.1f}% (ì£¼ì˜ í•„ìš”)")
        else:
            st.error(f"ë‚šì‹œì„± í™•ë¥ : {results['clickbait_prob']:.1f}% (ë†’ì€ ì£¼ì˜)")
    
    with col_detail2:
        st.write("**ğŸ˜Š ê°ì • ë¶„ì„**")
        st.write(f"ê°ì • ìœ í˜•: **{results['emotion']}**")
        if results['e_score'] < 30:
            st.success(f"ê°ì • ê°•ë„: {results['e_score']:.1f}ì  (ë§¤ìš° ê°ê´€ì )")
        elif results['e_score'] < 70:
            st.warning(f"ê°ì • ê°•ë„: {results['e_score']:.1f}ì  (ì ë‹¹íˆ ê°ì •ì )")
        else:
            st.error(f"ê°ì • ê°•ë„: {results['e_score']:.1f}ì  (ë§¤ìš° ê°ì •ì )")
    
    # ê¸°ì‚¬ ë‚´ìš©
    st.subheader("ğŸ“° ê¸°ì‚¬ ë‚´ìš©")
    st.write(f"**ì œëª©**: {results['title']}")
    st.write(f"**ì¶œì²˜**: {results['url']}")
    
    with st.expander("ğŸ“– ê¸°ì‚¬ ë³¸ë¬¸ ë³´ê¸° (ì¤‘ìš” ë¬¸ì¥ í•˜ì´ë¼ì´íŠ¸)"):
        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', results['text'])
        key_sentences = results.get('key_sentences', [])
        
        for sent in sentences:
            sent = sent.strip()
            if len(sent) > 5:
                # ì¤‘ìš” ë¬¸ì¥ì¸ì§€ í™•ì¸
                is_key = any(sent in key_sent for key_sent in key_sentences)
                
                if is_key:
                    st.markdown(f"""
                    <div style="background-color: #fef3c7; padding: 10px; 
                                border-radius: 5px; border-left: 4px solid #f59e0b;
                                margin-bottom: 8px;">
                        <strong style="color: #92400e;">â­ {sent}</strong>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown(f"""
                    <div style="background-color: #f9fafb; padding: 10px; 
                                border-radius: 5px; margin-bottom: 8px; color: #374151;">
                        {sent}
                    </div>
                    """, unsafe_allow_html=True)
    

# í‘¸í„°
st.divider()
st.caption("ğŸ¤– BERT ê¸°ë°˜ AI ëª¨ë¸ | ğŸ”¬ ë”¥ëŸ¬ë‹ ê¸°ìˆ  ì‚¬ìš© | âš¡ Powered by PyTorch & Transformers")